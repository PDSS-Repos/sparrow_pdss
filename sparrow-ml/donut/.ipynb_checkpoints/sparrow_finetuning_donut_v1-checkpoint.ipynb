{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\julian.smidek\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805f6368af844b869387b500a8b54ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d9e1a13f344f69bc25044f67cec52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/874k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec81df16c543498686ece69412abffa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/122k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e056f8dc23164b919381d49ad65c8023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f7156ce23344519676414579a12175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06445ccc9e6c467e9ebd2dac5bf357e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4826805f2d14a2398560440c57ad546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"juliansmidek/donut_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=121x155>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "image = example['image']\n",
    "# let's make the image a bit smaller when visualizing\n",
    "width, height = image.size\n",
    "display(image.resize((int(width*0.2), int(height*0.2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"gt_parse\": {\"certificate_of_origin\": \"CERTIFICATE OF ORIGIN\"}}\n"
     ]
    }
   ],
   "source": [
    "# let's load the corresponding JSON dictionary (as string representation)\n",
    "ground_truth = example['ground_truth']\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'certificate_of_origin': 'CERTIFICATE OF ORIGIN'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "literal_eval(ground_truth)['gt_parse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d25980fe57f4b7eba6ea386394e24b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\julian.smidek\\.cache\\huggingface\\hub\\models--naver-clova-ix--donut-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "max_length = 768\n",
    "image_size = [1280, 960]\n",
    "\n",
    "# update image_size of the encoder\n",
    "# during pre-training, a larger image size was used\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "config.encoder.image_size = image_size # (height, width)\n",
    "# update max_length of the decoder (for generation)\n",
    "config.decoder.max_length = max_length\n",
    "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
    "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bea62d85d94535bbefb10466ed170b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/362 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87301bc19b644c9b5c7e11f99ec7322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/518 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9762315d5274af6994adc38e7c508c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ae563bb4f9454a9ed44c6af7d3f116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500bd6c73aa34327afce5cd349b0dc84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/71.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9261cb508d451ab17fd2b7a0c5a9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385cb6cd3f95480494674bda6825c5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
    "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        max_length: the max number of tokens for the target sequences\n",
    "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "        prompt_end_token: the special token at the end of the sequences\n",
    "        sort_json_key: whether or not to sort the JSON keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + processor.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "\n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        pixel_values = processor(sample[\"image\"], random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "\n",
    "        # targets\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
    "        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "        return pixel_values, labels, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
    "processor.feature_extractor.size = image_size[::-1] # should be (width, height)\n",
    "processor.feature_extractor.do_align_long_axis = False\n",
    "\n",
    "train_dataset = DonutDataset(\"juliansmidek/donut_test\", max_length=max_length,\n",
    "                             split=\"train\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )\n",
    "\n",
    "val_dataset = DonutDataset(\"juliansmidek/donut_test\", max_length=max_length,\n",
    "                             split=\"validation\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57542"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57522"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pixel_values, labels, target_sequence = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>\n",
      "\n",
      "CER\n",
      "TIF\n",
      "ICA\n",
      "TE\n",
      "OF\n",
      "OR\n",
      "IG\n",
      "IN\n",
      "</s_certificate_of_origin>\n",
      "</s>\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "for id in labels.tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>CERTIFICATE OF ORIGIN</s_certificate_of_origin></s>\n"
     ]
    }
   ],
   "source": [
    "print(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token ID: <pad>\n",
      "Decoder start token ID: <s_cord-v2>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# feel free to increase the batch size if you have a lot of memory\n",
    "# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\n",
    "# Set num_workers=4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>\n",
      "\n",
      "CER\n",
      "IF\n",
      "ICA\n",
      "TE\n",
      "OF\n",
      "OR\n",
      "IG\n",
      "IN\n",
      "</s_certificate_of_origin>\n",
      "</s>\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n",
      "-100\n"
     ]
    }
   ],
   "source": [
    "for id in labels.squeeze().tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>CERTIFICATE OF ORIGIN</s_certificate_of_origin></s>\n"
     ]
    }
   ],
   "source": [
    "print(target_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define LightingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, labels, _ = batch\n",
    "\n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, labels, answers = batch\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        # we feed the prompt to the model\n",
    "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
    "\n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_input_ids,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "\n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = list()\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            # NOT NEEDED ANYMORE\n",
    "            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # I set this to 1 manually\n",
    "        # (previously set to len(self.config.dataset_name_or_paths))\n",
    "        num_of_loaders = 1\n",
    "        if num_of_loaders == 1:\n",
    "            validation_step_outputs = [validation_step_outputs]\n",
    "        assert len(validation_step_outputs) == num_of_loaders\n",
    "        cnt = [0] * num_of_loaders\n",
    "        total_metric = [0] * num_of_loaders\n",
    "        val_metric = [0] * num_of_loaders\n",
    "        for i, results in enumerate(validation_step_outputs):\n",
    "            for scores in results:\n",
    "                cnt[i] += len(scores)\n",
    "                total_metric[i] += np.sum(scores)\n",
    "            val_metric[i] = total_metric[i] / cnt[i]\n",
    "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
    "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
    "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO add scheduler\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set epochs = 30\n",
    "# Set num_training_samples_per_epoch = training set size\n",
    "config = {\"max_epochs\":30,\n",
    "          \"val_check_interval\":0.4, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\":1,\n",
    "          \"gradient_clip_val\":1.0,\n",
    "          \"num_training_samples_per_epoch\": 425,\n",
    "          \"lr\":3e-5,\n",
    "          \"train_batch_sizes\": [8],\n",
    "          \"val_batch_sizes\": [1],\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 81, # 425 / 8 = 54, 54 * 10 = 540, 540 * 0.15 = 81\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": False,\n",
    "          }\n",
    "\n",
    "model_module = DonutModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:751: UserWarning: You passed `Trainer(accelerator='cpu', precision=16)` but native AMP is not supported on CPU. Using `precision='bf16'` instead.\n",
      "  rank_zero_warn(\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | VisionEncoderDecoderModel | 201 M \n",
      "----------------------------------------------------\n",
      "201 M     Trainable params\n",
      "0         Non-trainable params\n",
      "201 M     Total params\n",
      "807.478   Total estimated model params size (MB)\n",
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1609: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\julian.smidek\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350a134b504045cbb04643c824eb5ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"Sparrow\", name=\"invoices-donut-v5\")\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(\"katanaml-org/invoices-donut-model-v1\",\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(\"juliansmidek/donut_test\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(\"juliansmidek/donut_test\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        val_check_interval=config.get(\"val_check_interval\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=16, # we'll use mixed precision\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[PushToHubCallback()],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"katanaml-org/invoices-donut-model-v1\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"katanaml-org/invoices-donut-model-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "dataset = load_dataset(\"katanaml-org/invoices-donut\", split=\"test\")\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    ground_truth = ground_truth[\"gt_parse\"]\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "\n",
    "    accs.append(score)\n",
    "    output_list.append(seq)\n",
    "\n",
    "scores = {\"accuracies\": accs, \"mean_accuracy\": np.mean(accs)}\n",
    "print(scores, f\"length : {len(accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Mean accuracy:\", np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
