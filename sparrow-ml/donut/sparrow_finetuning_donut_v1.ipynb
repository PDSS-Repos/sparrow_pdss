{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\thoma\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"juliansmidek/donut_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 7\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 1\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=121x155>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "image = example['image']\n",
    "# let's make the image a bit smaller when visualizing\n",
    "width, height = image.size\n",
    "display(image.resize((int(width*0.2), int(height*0.2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"gt_parse\": {\"certificate_of_origin\": \"CERTIFICATE OF ORIGIN\", \"bill_of_lading_number\": \"00LU2114644922\", \"importer_consignee\": \"TEXHONG RENZE MTILE DOINT STOCK COMPANY ADORESS: NHONTRACH 5 INDUSTRIAL ZONE, NHONTHACH, DONGNAL, VIETNAM.\", \"net_weight_kgs\": \"49,542.00 KGS\", \"gross_weight_kgs\": \"49,955.00 KGS\", \"number_of_bales\": \"230\", \"cotton_origin\": \"BALES OF BRAZILIAN RAW COTTON\", \"invoice_date\": \"23rd SEPTEMBER 2020\"}}\n"
     ]
    }
   ],
   "source": [
    "# let's load the corresponding JSON dictionary (as string representation)\n",
    "ground_truth = example['ground_truth']\n",
    "print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'certificate_of_origin': 'CERTIFICATE OF ORIGIN',\n",
       " 'bill_of_lading_number': '00LU2114644922',\n",
       " 'importer_consignee': 'TEXHONG RENZE MTILE DOINT STOCK COMPANY ADORESS: NHONTRACH 5 INDUSTRIAL ZONE, NHONTHACH, DONGNAL, VIETNAM.',\n",
       " 'net_weight_kgs': '49,542.00 KGS',\n",
       " 'gross_weight_kgs': '49,955.00 KGS',\n",
       " 'number_of_bales': '230',\n",
       " 'cotton_origin': 'BALES OF BRAZILIAN RAW COTTON',\n",
       " 'invoice_date': '23rd SEPTEMBER 2020'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "literal_eval(ground_truth)['gt_parse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Load model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderConfig\n",
    "\n",
    "max_length = 768\n",
    "image_size = [1280, 960]\n",
    "\n",
    "# update image_size of the encoder\n",
    "# during pre-training, a larger image size was used\n",
    "config = VisionEncoderDecoderConfig.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "config.encoder.image_size = image_size # (height, width)\n",
    "# update max_length of the decoder (for generation)\n",
    "config.decoder.max_length = max_length\n",
    "# TODO we should actually update max_position_embeddings and interpolate the pre-trained ones:\n",
    "# https://github.com/clovaai/donut/blob/0acc65a85d140852b8d9928565f0f6b2d98dc088/donut/model.py#L602"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel, BartConfig\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "added_tokens = []\n",
    "\n",
    "class DonutDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DonutDataset which is saved in huggingface datasets format. (see details in https://huggingface.co/docs/datasets)\n",
    "    Each row, consists of image path(png/jpg/jpeg) and gt data (json/jsonl/txt),\n",
    "    and it will be converted into input_tensor(vectorized image) and input_ids(tokenized string).\n",
    "    Args:\n",
    "        dataset_name_or_path: name of dataset (available at huggingface.co/datasets) or the path containing image files and metadata.jsonl\n",
    "        max_length: the max number of tokens for the target sequences\n",
    "        split: whether to load \"train\", \"validation\" or \"test\" split\n",
    "        ignore_id: ignore_index for torch.nn.CrossEntropyLoss\n",
    "        task_start_token: the special token to be fed to the decoder to conduct the target task\n",
    "        prompt_end_token: the special token at the end of the sequences\n",
    "        sort_json_key: whether or not to sort the JSON keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        max_length: int,\n",
    "        split: str = \"train\",\n",
    "        ignore_id: int = -100,\n",
    "        task_start_token: str = \"<s>\",\n",
    "        prompt_end_token: str = None,\n",
    "        sort_json_key: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.split = split\n",
    "        self.ignore_id = ignore_id\n",
    "        self.task_start_token = task_start_token\n",
    "        self.prompt_end_token = prompt_end_token if prompt_end_token else task_start_token\n",
    "        self.sort_json_key = sort_json_key\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.gt_token_sequences = []\n",
    "        for sample in self.dataset:\n",
    "            ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "            if \"gt_parses\" in ground_truth:  # when multiple ground truths are available, e.g., docvqa\n",
    "                assert isinstance(ground_truth[\"gt_parses\"], list)\n",
    "                gt_jsons = ground_truth[\"gt_parses\"]\n",
    "            else:\n",
    "                assert \"gt_parse\" in ground_truth and isinstance(ground_truth[\"gt_parse\"], dict)\n",
    "                gt_jsons = [ground_truth[\"gt_parse\"]]\n",
    "\n",
    "            self.gt_token_sequences.append(\n",
    "                [\n",
    "                    self.json2token(\n",
    "                        gt_json,\n",
    "                        update_special_tokens_for_json_key=self.split == \"train\",\n",
    "                        sort_json_key=self.sort_json_key,\n",
    "                    )\n",
    "                    + processor.tokenizer.eos_token\n",
    "                    for gt_json in gt_jsons  # load json from list of json\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.add_tokens([self.task_start_token, self.prompt_end_token])\n",
    "        self.prompt_end_token_id = processor.tokenizer.convert_tokens_to_ids(self.prompt_end_token)\n",
    "\n",
    "    def json2token(self, obj: Any, update_special_tokens_for_json_key: bool = True, sort_json_key: bool = True):\n",
    "        \"\"\"\n",
    "        Convert an ordered JSON object into a token sequence\n",
    "        \"\"\"\n",
    "        if type(obj) == dict:\n",
    "            if len(obj) == 1 and \"text_sequence\" in obj:\n",
    "                return obj[\"text_sequence\"]\n",
    "            else:\n",
    "                output = \"\"\n",
    "                if sort_json_key:\n",
    "                    keys = sorted(obj.keys(), reverse=True)\n",
    "                else:\n",
    "                    keys = obj.keys()\n",
    "                for k in keys:\n",
    "                    if update_special_tokens_for_json_key:\n",
    "                        self.add_tokens([fr\"<s_{k}>\", fr\"</s_{k}>\"])\n",
    "                    output += (\n",
    "                        fr\"<s_{k}>\"\n",
    "                        + self.json2token(obj[k], update_special_tokens_for_json_key, sort_json_key)\n",
    "                        + fr\"</s_{k}>\"\n",
    "                    )\n",
    "                return output\n",
    "        elif type(obj) == list:\n",
    "            return r\"<sep/>\".join(\n",
    "                [self.json2token(item, update_special_tokens_for_json_key, sort_json_key) for item in obj]\n",
    "            )\n",
    "        else:\n",
    "            obj = str(obj)\n",
    "            if f\"<{obj}/>\" in added_tokens:\n",
    "                obj = f\"<{obj}/>\"  # for categorical special tokens\n",
    "            return obj\n",
    "\n",
    "    def add_tokens(self, list_of_tokens: List[str]):\n",
    "        \"\"\"\n",
    "        Add special tokens to tokenizer and resize the token embeddings of the decoder\n",
    "        \"\"\"\n",
    "        newly_added_num = processor.tokenizer.add_tokens(list_of_tokens)\n",
    "        if newly_added_num > 0:\n",
    "            model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "            added_tokens.extend(list_of_tokens)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Load image from image_path of given dataset_path and convert into input_tensor and labels\n",
    "        Convert gt data into input_ids (tokenized string)\n",
    "        Returns:\n",
    "            input_tensor : preprocessed image\n",
    "            input_ids : tokenized gt_data\n",
    "            labels : masked labels (model doesn't need to predict prompt and pad token)\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        pixel_values = processor(sample[\"image\"], random_padding=self.split == \"train\", return_tensors=\"pt\").pixel_values\n",
    "        pixel_values = pixel_values.squeeze()\n",
    "\n",
    "        # targets\n",
    "        target_sequence = random.choice(self.gt_token_sequences[idx])  # can be more than one, e.g., DocVQA Task 1\n",
    "        input_ids = processor.tokenizer(\n",
    "            target_sequence,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"].squeeze(0)\n",
    "\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == processor.tokenizer.pad_token_id] = self.ignore_id  # model doesn't need to predict pad token\n",
    "        # labels[: torch.nonzero(labels == self.prompt_end_token_id).sum() + 1] = self.ignore_id  # model doesn't need to predict prompt (for VQA)\n",
    "        return pixel_values, labels, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\Documents\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\transformers\\models\\donut\\processing_donut.py:190: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# we update some settings which differ from pretraining; namely the size of the images + no rotation required\n",
    "# source: https://github.com/clovaai/donut/blob/master/config/train_cord.yaml\n",
    "processor.feature_extractor.size = image_size[::-1] # should be (width, height)\n",
    "processor.feature_extractor.do_align_long_axis = False\n",
    "\n",
    "train_dataset = DonutDataset(\"juliansmidek/donut_test\", max_length=max_length,\n",
    "                             split=\"train\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )\n",
    "\n",
    "val_dataset = DonutDataset(\"juliansmidek/donut_test\", max_length=max_length,\n",
    "                             split=\"validation\", task_start_token=\"<s_cord-v2>\", prompt_end_token=\"<s_cord-v2>\",\n",
    "                             sort_json_key=False, # cord dataset is preprocessed, so no need for this\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(added_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57542"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57522"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pixel_values, labels, target_sequence = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>\n",
      "\n",
      "CER\n",
      "TIF\n",
      "ICA\n",
      "TE\n",
      "OF\n",
      "OR\n",
      "IG\n",
      "IN\n",
      "</s_certificate_of_origin>\n",
      "<s_bill_of_lading_number>\n",
      "00\n",
      "LU\n",
      "2\n",
      "11\n",
      "464\n",
      "49\n",
      "22\n",
      "</s_bill_of_lading_number>\n",
      "<s_importer_consignee>\n",
      "T\n",
      "EX\n",
      "HO\n",
      "NG\n",
      "\n",
      "REN\n",
      "ZE\n",
      "MT\n",
      "ILE\n"
     ]
    }
   ],
   "source": [
    "for id in labels.tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>CERTIFICATE OF ORIGIN</s_certificate_of_origin><s_bill_of_lading_number>00LU2114644922</s_bill_of_lading_number><s_importer_consignee>TEXHONG RENZE MTILE DOINT STOCK COMPANY ADORESS: NHONTRACH 5 INDUSTRIAL ZONE, NHONTHACH, DONGNAL, VIETNAM.</s_importer_consignee><s_net_weight_kgs>49,542.00 KGS</s_net_weight_kgs><s_gross_weight_kgs>49,955.00 KGS</s_gross_weight_kgs><s_number_of_bales>230</s_number_of_bales><s_cotton_origin>BALES OF BRAZILIAN RAW COTTON</s_cotton_origin><s_invoice_date>23rd SEPTEMBER 2020</s_invoice_date></s>\n"
     ]
    }
   ],
   "source": [
    "print(target_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids(['<s_cord-v2>'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token ID: <pad>\n",
      "Decoder start token ID: <s_cord-v2>\n"
     ]
    }
   ],
   "source": [
    "print(\"Pad token ID:\", processor.decode([model.config.pad_token_id]))\n",
    "print(\"Decoder start token ID:\", processor.decode([model.config.decoder_start_token_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# feel free to increase the batch size if you have a lot of memory\n",
    "# I'm fine-tuning on Colab and given the large image size, batch size > 1 is not feasible\n",
    "# Set num_workers=4\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True,num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>\n",
      "\n",
      "CER\n",
      "TIF\n",
      "ICA\n",
      "TE\n",
      "OF\n",
      "OR\n",
      "IG\n",
      "IN\n",
      "</s_certificate_of_origin>\n",
      "<s_bill_of_lading_number>\n",
      "00\n",
      "LU\n",
      "2\n",
      "11\n",
      "464\n",
      "49\n",
      "22\n",
      "</s_bill_of_lading_number>\n",
      "<s_importer_consignee>\n",
      "T\n",
      "EX\n",
      "HO\n",
      "NG\n",
      "\n",
      "REN\n",
      "ZE\n",
      "MT\n",
      "ILE\n"
     ]
    }
   ],
   "source": [
    "for id in labels.squeeze().tolist()[:30]:\n",
    "  if id != -100:\n",
    "    print(processor.decode([id]))\n",
    "  else:\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1280, 960])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dataloader))\n",
    "pixel_values, labels, target_sequences = batch\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s_certificate_of_origin>CERTIFICATE OF ORIGIN</s_certificate_of_origin><s_importer_consignee>BROTEX (VIETNAM) CO. LTD,</s_importer_consignee><s_bill_of_lading_number>COSU8025133350</s_bill_of_lading_number><s_number_of_bales>1439</s_number_of_bales><s_cotton_origin>BALES OF BRAZIL RAW COTTON</s_cotton_origin><s_net_weight_kgs>290,360.600 KGS</s_net_weight_kgs><s_gross_weight_kgs>292,663.000 KGS</s_gross_weight_kgs><s_invoice_date>FEBRUARY 12TH, 2020</s_invoice_date></s>\n"
     ]
    }
   ],
   "source": [
    "print(target_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Define LightingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import rank_zero_only\n",
    "\n",
    "\n",
    "class DonutModelPLModule(pl.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        pixel_values, labels, _ = batch\n",
    "\n",
    "        outputs = self.model(pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        self.log_dict({\"train_loss\": loss}, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        pixel_values, labels, answers = batch\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        # we feed the prompt to the model\n",
    "        decoder_input_ids = torch.full((batch_size, 1), self.model.config.decoder_start_token_id, device=self.device)\n",
    "\n",
    "        outputs = self.model.generate(pixel_values,\n",
    "                                   decoder_input_ids=decoder_input_ids,\n",
    "                                   max_length=max_length,\n",
    "                                   early_stopping=True,\n",
    "                                   pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                                   eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                                   use_cache=True,\n",
    "                                   num_beams=1,\n",
    "                                   bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n",
    "                                   return_dict_in_generate=True,)\n",
    "\n",
    "        predictions = []\n",
    "        for seq in self.processor.tokenizer.batch_decode(outputs.sequences):\n",
    "            seq = seq.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n",
    "            seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "            predictions.append(seq)\n",
    "\n",
    "        scores = list()\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            # NOT NEEDED ANYMORE\n",
    "            # answer = re.sub(r\"<.*?>\", \"\", answer, count=1)\n",
    "            answer = answer.replace(self.processor.tokenizer.eos_token, \"\")\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # I set this to 1 manually\n",
    "        # (previously set to len(self.config.dataset_name_or_paths))\n",
    "        num_of_loaders = 1\n",
    "        if num_of_loaders == 1:\n",
    "            validation_step_outputs = [validation_step_outputs]\n",
    "        assert len(validation_step_outputs) == num_of_loaders\n",
    "        cnt = [0] * num_of_loaders\n",
    "        total_metric = [0] * num_of_loaders\n",
    "        val_metric = [0] * num_of_loaders\n",
    "        for i, results in enumerate(validation_step_outputs):\n",
    "            for scores in results:\n",
    "                cnt[i] += len(scores)\n",
    "                total_metric[i] += np.sum(scores)\n",
    "            val_metric[i] = total_metric[i] / cnt[i]\n",
    "            val_metric_name = f\"val_metric_{i}th_dataset\"\n",
    "            self.log_dict({val_metric_name: val_metric[i]}, sync_dist=True)\n",
    "        self.log_dict({\"val_metric\": np.sum(total_metric) / np.sum(cnt)}, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # TODO add scheduler\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set epochs = 30\n",
    "# Set num_training_samples_per_epoch = training set size\n",
    "config = {\"max_epochs\":1,\n",
    "          \"val_check_interval\":0.4, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\":1,\n",
    "          \"gradient_clip_val\":1.0,\n",
    "          \"num_training_samples_per_epoch\": 425,\n",
    "          \"lr\":3e-5,\n",
    "          \"train_batch_sizes\": [8],\n",
    "          \"val_batch_sizes\": [1],\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 81, # 425 / 8 = 54, 54 * 10 = 540, 540 * 0.15 = 81\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": False,\n",
    "          }\n",
    "\n",
    "model_module = DonutModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                      | Params\n",
      "----------------------------------------------------\n",
      "0 | model | VisionEncoderDecoderModel | 201 M \n",
      "----------------------------------------------------\n",
      "201 M     Trainable params\n",
      "0         Non-trainable params\n",
      "201 M     Total params\n",
      "807.478   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|   | 2/10 [2:46:11<11:04:47, 4985.92s/it, loss=8.94, v_num=8xv2]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thoma\\Documents\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\transformers\\generation\\utils.py:1553: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "C:\\Users\\thoma\\Documents\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  30%|   | 3/10 [2:52:07<6:41:37, 3442.56s/it, loss=8.94, v_num=8xv2]\u001b[A\n",
      "Epoch 0:  50%|  | 5/10 [5:37:36<5:37:36, 4051.27s/it, loss=8.39, v_num=8xv2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  60%|  | 6/10 [5:43:33<3:49:02, 3435.52s/it, loss=8.39, v_num=8xv2]\u001b[A\n",
      "Epoch 0:  80%| | 8/10 [8:28:54<2:07:13, 3816.80s/it, loss=8.29, v_num=8xv2]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                        | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  90%|| 9/10 [8:34:49<57:12, 3432.19s/it, loss=8.29, v_num=8xv2]\u001b[A\n",
      "Epoch 0: 100%|| 10/10 [9:58:00<00:00, 3588.02s/it, loss=8.03, v_num=8xv2]\u001b[APushing model to the hub, epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'decoder.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/809M [00:00<?, ?B/s]\u001b[A\n",
      "model.safetensors:   0%|                  | 16.4k/809M [00:00<1:30:56, 148kB/s]\u001b[A\n",
      "model.safetensors:   0%|                    | 672k/809M [00:00<04:23, 3.07MB/s]\u001b[A\n",
      "model.safetensors:   0%|                    | 967k/809M [00:00<10:49, 1.24MB/s]\u001b[A\n",
      "model.safetensors:   0%|                   | 1.93M/809M [00:00<04:44, 2.84MB/s]\u001b[A\n",
      "model.safetensors:   0%|                   | 3.15M/809M [00:00<02:54, 4.62MB/s]\u001b[A\n",
      "model.safetensors:   0%|                   | 3.83M/809M [00:01<02:36, 5.13MB/s]\u001b[A\n",
      "model.safetensors:   1%|                   | 4.52M/809M [00:01<02:28, 5.43MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 5.44M/809M [00:01<02:06, 6.37MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 6.39M/809M [00:01<01:57, 6.84MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 7.44M/809M [00:01<01:53, 7.07MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 8.21M/809M [00:01<01:56, 6.88MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 9.03M/809M [00:01<01:53, 7.03MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 9.76M/809M [00:01<01:52, 7.12MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 10.5M/809M [00:01<01:55, 6.90MB/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 11.3M/809M [00:02<01:58, 6.76MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 12.2M/809M [00:02<01:53, 7.02MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 13.3M/809M [00:02<01:41, 7.85MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 14.4M/809M [00:02<01:33, 8.53MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 16.0M/809M [00:02<02:14, 5.88MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 17.0M/809M [00:02<02:03, 6.43MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 18.0M/809M [00:03<01:51, 7.07MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 19.1M/809M [00:03<01:39, 7.97MB/s]\u001b[A\n",
      "model.safetensors:   2%|                  | 20.1M/809M [00:03<01:36, 8.21MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 21.0M/809M [00:03<01:32, 8.49MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 22.2M/809M [00:03<01:25, 9.23MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 23.5M/809M [00:03<01:21, 9.64MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 24.5M/809M [00:04<03:29, 3.74MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 25.3M/809M [00:04<03:21, 3.90MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 26.0M/809M [00:04<03:04, 4.24MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 26.9M/809M [00:04<02:34, 5.07MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 28.4M/809M [00:04<01:55, 6.76MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 29.6M/809M [00:04<01:40, 7.73MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 31.0M/809M [00:04<01:26, 9.04MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 32.1M/809M [00:05<02:33, 5.07MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 33.2M/809M [00:05<02:10, 5.93MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 34.7M/809M [00:05<01:41, 7.65MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 35.8M/809M [00:05<01:33, 8.29MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 37.4M/809M [00:05<01:18, 9.87MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 38.6M/809M [00:05<01:21, 9.46MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 39.7M/809M [00:06<01:46, 7.23MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 40.6M/809M [00:06<03:42, 3.46MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 41.3M/809M [00:07<03:32, 3.61MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 41.9M/809M [00:07<03:54, 3.28MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 43.1M/809M [00:07<02:48, 4.54MB/s]\u001b[A\n",
      "model.safetensors:   6%|                  | 44.6M/809M [00:07<02:07, 6.02MB/s]\u001b[A\n",
      "model.safetensors:   6%|                  | 45.7M/809M [00:07<01:49, 6.98MB/s]\u001b[A\n",
      "model.safetensors:   6%|                  | 47.1M/809M [00:07<01:30, 8.41MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 48.2M/809M [00:08<02:25, 5.23MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 49.0M/809M [00:08<02:11, 5.76MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 50.0M/809M [00:08<01:58, 6.43MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 51.3M/809M [00:08<01:39, 7.62MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 52.3M/809M [00:08<01:35, 7.90MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 53.5M/809M [00:08<01:51, 6.78MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 54.8M/809M [00:08<01:38, 7.69MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 56.1M/809M [00:09<01:30, 8.34MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 57.0M/809M [00:09<03:34, 3.51MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 57.7M/809M [00:09<03:15, 3.84MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 59.1M/809M [00:10<02:25, 5.14MB/s]\u001b[A\n",
      "model.safetensors:   7%|                 | 60.5M/809M [00:10<01:55, 6.49MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 61.5M/809M [00:10<02:16, 5.49MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 62.9M/809M [00:10<01:45, 7.05MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 64.0M/809M [00:10<02:36, 4.77MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 65.3M/809M [00:11<02:07, 5.83MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 66.6M/809M [00:11<01:49, 6.81MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 67.8M/809M [00:11<01:42, 7.26MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 68.9M/809M [00:11<01:37, 7.60MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 69.8M/809M [00:11<01:56, 6.36MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 70.9M/809M [00:11<01:45, 7.00MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 72.0M/809M [00:11<01:34, 7.77MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 72.9M/809M [00:12<03:55, 3.13MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 73.9M/809M [00:12<03:10, 3.86MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 75.0M/809M [00:12<02:30, 4.89MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 76.1M/809M [00:12<02:05, 5.82MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 77.0M/809M [00:13<01:53, 6.42MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 78.2M/809M [00:13<01:40, 7.26MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 79.1M/809M [00:13<01:42, 7.09MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 80.0M/809M [00:13<02:40, 4.54MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 81.2M/809M [00:13<02:06, 5.76MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 82.3M/809M [00:13<01:51, 6.53MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 83.7M/809M [00:14<01:36, 7.48MB/s]\u001b[A\n",
      "model.safetensors:  11%|                 | 85.4M/809M [00:14<01:23, 8.65MB/s]\u001b[A\n",
      "model.safetensors:  11%|                 | 86.7M/809M [00:14<01:17, 9.26MB/s]\u001b[A\n",
      "model.safetensors:  11%|                 | 88.2M/809M [00:14<01:13, 9.80MB/s]\u001b[A\n",
      "model.safetensors:  11%|                 | 89.2M/809M [00:15<03:25, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  11%|                 | 90.0M/809M [00:15<03:05, 3.87MB/s]\u001b[A\n",
      "model.safetensors:  11%|                | 91.1M/809M [00:15<02:40, 4.48MB/s]\u001b[A\n",
      "model.safetensors:  11%|                | 92.3M/809M [00:15<02:31, 4.74MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 93.7M/809M [00:15<01:56, 6.15MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 94.7M/809M [00:16<01:46, 6.70MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 95.9M/809M [00:16<01:31, 7.81MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 96.9M/809M [00:16<02:52, 4.13MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 98.2M/809M [00:16<02:22, 5.00MB/s]\u001b[A\n",
      "model.safetensors:  12%|                 | 100M/809M [00:16<01:42, 6.89MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 102M/809M [00:17<01:24, 8.37MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 103M/809M [00:17<01:13, 9.55MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  13%|                 | 104M/809M [00:17<01:23, 8.48MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 105M/809M [00:18<03:38, 3.22MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 106M/809M [00:18<03:15, 3.59MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 107M/809M [00:18<02:44, 4.28MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 109M/809M [00:18<02:03, 5.68MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 110M/809M [00:18<01:40, 6.94MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 112M/809M [00:18<01:23, 8.40MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 113M/809M [00:19<02:44, 4.24MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 114M/809M [00:19<02:03, 5.62MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 116M/809M [00:19<01:40, 6.92MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 118M/809M [00:19<01:23, 8.24MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 120M/809M [00:20<01:10, 9.79MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 121M/809M [00:21<03:03, 3.75MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 122M/809M [00:21<03:20, 3.43MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 123M/809M [00:21<02:30, 4.57MB/s]\u001b[A\n",
      "model.safetensors:  15%|                 | 125M/809M [00:21<01:49, 6.22MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 127M/809M [00:21<01:50, 6.18MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 128M/809M [00:21<01:35, 7.14MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 129M/809M [00:22<02:10, 5.21MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 130M/809M [00:22<02:25, 4.69MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 132M/809M [00:22<01:44, 6.49MB/s]\u001b[A\n",
      "model.safetensors:  16%|                | 133M/809M [00:22<01:20, 8.44MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 135M/809M [00:22<01:14, 9.04MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 136M/809M [00:22<01:04, 10.4MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 137M/809M [00:23<02:55, 3.82MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 138M/809M [00:23<02:34, 4.35MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 139M/809M [00:24<02:15, 4.94MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 140M/809M [00:24<01:59, 5.59MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 142M/809M [00:24<01:31, 7.29MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 143M/809M [00:24<01:20, 8.29MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 144M/809M [00:24<02:34, 4.31MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 146M/809M [00:25<01:51, 5.93MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 147M/809M [00:25<01:28, 7.45MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 149M/809M [00:25<01:37, 6.80MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 150M/809M [00:25<01:25, 7.74MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 151M/809M [00:25<01:23, 7.87MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 152M/809M [00:25<01:18, 8.35MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 153M/809M [00:26<03:04, 3.56MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 154M/809M [00:26<03:21, 3.25MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 156M/809M [00:26<02:16, 4.77MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 157M/809M [00:27<01:50, 5.89MB/s]\u001b[A\n",
      "model.safetensors:  20%|                | 159M/809M [00:27<01:21, 7.96MB/s]\u001b[A\n",
      "model.safetensors:  20%|                | 160M/809M [00:27<02:40, 4.04MB/s]\u001b[A\n",
      "model.safetensors:  20%|                | 162M/809M [00:27<01:55, 5.62MB/s]\u001b[A\n",
      "model.safetensors:  20%|                | 163M/809M [00:28<01:38, 6.56MB/s]\u001b[A\n",
      "model.safetensors:  20%|                | 165M/809M [00:28<01:27, 7.38MB/s]\u001b[A\n",
      "model.safetensors:  21%|                | 166M/809M [00:28<01:15, 8.50MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 168M/809M [00:28<01:13, 8.77MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 169M/809M [00:29<03:09, 3.38MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 170M/809M [00:29<02:42, 3.94MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 171M/809M [00:29<02:18, 4.62MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 172M/809M [00:29<01:54, 5.55MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 174M/809M [00:29<01:29, 7.09MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 175M/809M [00:29<01:11, 8.91MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 177M/809M [00:30<02:10, 4.85MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 178M/809M [00:30<01:48, 5.83MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 180M/809M [00:30<01:21, 7.77MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 181M/809M [00:30<01:10, 8.94MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 183M/809M [00:31<00:58, 10.7MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 185M/809M [00:32<03:06, 3.34MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 186M/809M [00:32<02:50, 3.65MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 187M/809M [00:32<02:35, 4.01MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 187M/809M [00:32<02:19, 4.44MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 189M/809M [00:32<01:53, 5.46MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 190M/809M [00:32<01:31, 6.76MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 191M/809M [00:33<01:40, 6.15MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 192M/809M [00:33<01:36, 6.41MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 193M/809M [00:33<02:31, 4.06MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 194M/809M [00:33<02:06, 4.88MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 195M/809M [00:33<02:01, 5.06MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 195M/809M [00:34<01:54, 5.37MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 197M/809M [00:34<01:23, 7.32MB/s]\u001b[A\n",
      "model.safetensors:  24%|               | 198M/809M [00:34<01:28, 6.92MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 199M/809M [00:34<01:13, 8.29MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 200M/809M [00:34<01:14, 8.22MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 202M/809M [00:34<00:54, 11.2MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 203M/809M [00:34<00:53, 11.4MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 205M/809M [00:34<00:53, 11.2MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 206M/809M [00:34<00:48, 12.5MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 208M/809M [00:35<00:48, 12.5MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 209M/809M [00:35<01:19, 7.51MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 210M/809M [00:35<01:06, 9.00MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 212M/809M [00:35<01:00, 9.83MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 213M/809M [00:35<00:53, 11.2MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 215M/809M [00:35<00:50, 11.7MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 216M/809M [00:36<01:02, 9.44MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 217M/809M [00:36<02:45, 3.58MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 218M/809M [00:37<02:28, 3.97MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 219M/809M [00:37<02:12, 4.44MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 221M/809M [00:37<01:38, 5.96MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 222M/809M [00:37<01:15, 7.76MB/s]\u001b[A\n",
      "model.safetensors:  28%|              | 224M/809M [00:37<02:02, 4.77MB/s]\u001b[A\n",
      "model.safetensors:  28%|              | 226M/809M [00:38<01:32, 6.28MB/s]\u001b[A\n",
      "model.safetensors:  28%|              | 228M/809M [00:38<01:12, 8.00MB/s]\u001b[A\n",
      "model.safetensors:  28%|              | 229M/809M [00:38<01:01, 9.35MB/s]\u001b[A\n",
      "model.safetensors:  29%|              | 231M/809M [00:38<00:59, 9.79MB/s]\u001b[A\n",
      "model.safetensors:  29%|              | 232M/809M [00:38<00:51, 11.1MB/s]\u001b[A\n",
      "model.safetensors:  29%|              | 234M/809M [00:38<00:51, 11.3MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  29%|              | 236M/809M [00:38<00:47, 12.1MB/s]\u001b[A\n",
      "model.safetensors:  29%|              | 238M/809M [00:38<00:42, 13.3MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 240M/809M [00:39<00:39, 14.5MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 241M/809M [00:39<01:04, 8.86MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 243M/809M [00:39<00:58, 9.67MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 245M/809M [00:39<00:50, 11.2MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 246M/809M [00:39<01:01, 9.19MB/s]\u001b[A\n",
      "model.safetensors:  31%|              | 247M/809M [00:39<00:57, 9.76MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 249M/809M [00:40<00:59, 9.48MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 250M/809M [00:41<02:55, 3.19MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 250M/809M [00:41<02:40, 3.47MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 251M/809M [00:41<02:26, 3.82MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 252M/809M [00:41<02:07, 4.35MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 253M/809M [00:41<01:43, 5.35MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 254M/809M [00:41<01:27, 6.33MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 255M/809M [00:41<01:23, 6.67MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 256M/809M [00:42<02:34, 3.59MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 258M/809M [00:42<01:34, 5.82MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 259M/809M [00:42<01:23, 6.57MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 261M/809M [00:42<01:09, 7.91MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 262M/809M [00:42<00:56, 9.60MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 264M/809M [00:43<01:19, 6.85MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 265M/809M [00:44<03:08, 2.89MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 266M/809M [00:44<02:56, 3.08MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 266M/809M [00:44<02:47, 3.25MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 267M/809M [00:44<02:23, 3.78MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 268M/809M [00:44<01:49, 4.95MB/s]\u001b[A\n",
      "model.safetensors:  33%|             | 270M/809M [00:44<01:19, 6.75MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 271M/809M [00:45<01:07, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 272M/809M [00:45<02:24, 3.72MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 274M/809M [00:45<01:42, 5.24MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 276M/809M [00:45<01:17, 6.92MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 278M/809M [00:46<01:07, 7.92MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 279M/809M [00:46<00:59, 8.87MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 280M/809M [00:46<01:05, 8.04MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 281M/809M [00:47<02:36, 3.37MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 282M/809M [00:47<02:22, 3.70MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 283M/809M [00:47<02:09, 4.08MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 283M/809M [00:47<01:56, 4.52MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 285M/809M [00:47<01:31, 5.75MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 286M/809M [00:47<01:08, 7.64MB/s]\u001b[A\n",
      "model.safetensors:  36%|             | 288M/809M [00:47<00:56, 9.18MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 289M/809M [00:48<01:43, 5.01MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 290M/809M [00:48<01:30, 5.76MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 292M/809M [00:48<01:08, 7.57MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 293M/809M [00:48<01:02, 8.32MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 294M/809M [00:48<00:58, 8.86MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 296M/809M [00:49<00:54, 9.43MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 297M/809M [00:49<00:57, 8.96MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 298M/809M [00:50<02:45, 3.09MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 298M/809M [00:50<02:29, 3.42MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 300M/809M [00:50<02:02, 4.17MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 301M/809M [00:50<01:41, 5.02MB/s]\u001b[A\n",
      "model.safetensors:  37%|            | 302M/809M [00:50<01:17, 6.51MB/s]\u001b[A\n",
      "model.safetensors:  38%|            | 304M/809M [00:51<01:46, 4.75MB/s]\u001b[A\n",
      "model.safetensors:  38%|            | 306M/809M [00:51<01:18, 6.39MB/s]\u001b[A\n",
      "model.safetensors:  38%|            | 308M/809M [00:51<01:01, 8.15MB/s]\u001b[A\n",
      "model.safetensors:  38%|            | 310M/809M [00:51<00:50, 9.86MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 312M/809M [00:51<00:44, 11.3MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 313M/809M [00:52<02:19, 3.55MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 314M/809M [00:53<02:17, 3.60MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 315M/809M [00:53<02:04, 3.97MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 316M/809M [00:53<01:41, 4.86MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 318M/809M [00:53<01:15, 6.51MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 319M/809M [00:53<01:17, 6.35MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 320M/809M [00:54<02:09, 3.76MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 322M/809M [00:54<01:27, 5.56MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 324M/809M [00:54<01:10, 6.92MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 325M/809M [00:54<00:56, 8.50MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 327M/809M [00:54<00:48, 9.92MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 329M/809M [00:54<00:40, 11.7MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 331M/809M [00:54<00:38, 12.5MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 332M/809M [00:55<00:37, 12.8MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 334M/809M [00:55<00:35, 13.4MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 335M/809M [00:55<00:37, 12.6MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 337M/809M [00:55<01:04, 7.35MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 338M/809M [00:55<00:57, 8.21MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 339M/809M [00:55<00:53, 8.73MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 340M/809M [00:56<00:54, 8.61MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 342M/809M [00:56<00:45, 10.2MB/s]\u001b[A\n",
      "model.safetensors:  42%|           | 344M/809M [00:56<00:47, 9.80MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 345M/809M [00:57<01:59, 3.90MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 346M/809M [00:57<02:01, 3.81MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 347M/809M [00:57<01:39, 4.65MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 348M/809M [00:57<01:23, 5.52MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 349M/809M [00:57<01:20, 5.74MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 350M/809M [00:57<01:24, 5.46MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 351M/809M [00:58<01:11, 6.45MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 352M/809M [00:58<01:04, 7.06MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 353M/809M [00:58<01:30, 5.02MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 354M/809M [00:58<01:14, 6.10MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 355M/809M [00:58<01:04, 7.00MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 356M/809M [00:58<00:58, 7.68MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 358M/809M [00:58<00:51, 8.85MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 359M/809M [00:58<00:48, 9.21MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 360M/809M [00:59<00:45, 9.87MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 361M/809M [00:59<02:10, 3.43MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 362M/809M [01:00<02:01, 3.69MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  45%|           | 362M/809M [01:00<01:47, 4.14MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 363M/809M [01:00<01:37, 4.56MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 364M/809M [01:00<01:43, 4.30MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 366M/809M [01:00<01:09, 6.41MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 366M/809M [01:00<01:06, 6.67MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 367M/809M [01:00<01:01, 7.18MB/s]\u001b[A\n",
      "model.safetensors:  46%|           | 368M/809M [01:01<01:36, 4.55MB/s]\u001b[A\n",
      "model.safetensors:  46%|          | 369M/809M [01:01<01:36, 4.55MB/s]\u001b[A\n",
      "model.safetensors:  46%|          | 371M/809M [01:01<01:08, 6.37MB/s]\u001b[A\n",
      "model.safetensors:  46%|          | 372M/809M [01:01<00:58, 7.46MB/s]\u001b[A\n",
      "model.safetensors:  46%|          | 374M/809M [01:01<00:53, 8.11MB/s]\u001b[A\n",
      "model.safetensors:  46%|          | 376M/809M [01:01<00:45, 9.62MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 377M/809M [01:02<01:56, 3.71MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 378M/809M [01:02<01:39, 4.35MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 379M/809M [01:03<01:29, 4.83MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 380M/809M [01:03<01:15, 5.72MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 382M/809M [01:03<01:03, 6.70MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 383M/809M [01:03<00:50, 8.40MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 384M/809M [01:03<01:26, 4.94MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 386M/809M [01:04<01:08, 6.20MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 387M/809M [01:04<00:56, 7.51MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 389M/809M [01:04<00:52, 8.01MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 390M/809M [01:04<00:50, 8.23MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 391M/809M [01:04<00:46, 8.97MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 392M/809M [01:04<00:44, 9.31MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 393M/809M [01:05<01:58, 3.51MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 394M/809M [01:05<02:10, 3.19MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 395M/809M [01:06<01:55, 3.59MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 396M/809M [01:06<01:30, 4.56MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 397M/809M [01:06<01:16, 5.36MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 399M/809M [01:06<01:00, 6.73MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 400M/809M [01:06<00:54, 7.49MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 401M/809M [01:06<01:14, 5.50MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 402M/809M [01:07<01:20, 5.08MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 403M/809M [01:07<01:03, 6.36MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 404M/809M [01:07<00:50, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 406M/809M [01:07<00:41, 9.82MB/s]\u001b[A\n",
      "model.safetensors:  50%|          | 408M/809M [01:07<00:37, 10.8MB/s]\u001b[A\n",
      "model.safetensors:  51%|          | 409M/809M [01:08<01:38, 4.08MB/s]\u001b[A\n",
      "model.safetensors:  51%|         | 410M/809M [01:08<01:38, 4.06MB/s]\u001b[A\n",
      "model.safetensors:  51%|         | 411M/809M [01:08<01:25, 4.66MB/s]\u001b[A\n",
      "model.safetensors:  51%|         | 413M/809M [01:08<01:01, 6.41MB/s]\u001b[A\n",
      "model.safetensors:  51%|         | 414M/809M [01:08<00:50, 7.87MB/s]\u001b[A\n",
      "model.safetensors:  51%|         | 416M/809M [01:08<00:43, 8.99MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 417M/809M [01:09<01:21, 4.83MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 419M/809M [01:09<01:15, 5.14MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 420M/809M [01:09<01:00, 6.47MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 422M/809M [01:10<00:48, 8.01MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 424M/809M [01:10<00:41, 9.36MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 425M/809M [01:10<01:37, 3.92MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 426M/809M [01:11<01:28, 4.33MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 427M/809M [01:11<01:24, 4.55MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 427M/809M [01:11<01:27, 4.35MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 428M/809M [01:11<01:11, 5.34MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 430M/809M [01:11<00:54, 6.98MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 432M/809M [01:11<00:40, 9.23MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 433M/809M [01:12<01:15, 4.99MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 435M/809M [01:12<00:58, 6.43MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 436M/809M [01:12<00:49, 7.57MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 438M/809M [01:12<00:43, 8.62MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 439M/809M [01:12<00:40, 9.16MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 440M/809M [01:13<00:59, 6.25MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 441M/809M [01:14<02:17, 2.68MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 442M/809M [01:14<02:18, 2.65MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 442M/809M [01:14<01:59, 3.07MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 444M/809M [01:14<01:31, 4.01MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 445M/809M [01:14<01:06, 5.47MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 447M/809M [01:14<00:51, 7.08MB/s]\u001b[A\n",
      "model.safetensors:  55%|         | 448M/809M [01:15<01:46, 3.40MB/s]\u001b[A\n",
      "model.safetensors:  56%|         | 450M/809M [01:15<01:18, 4.58MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 451M/809M [01:15<01:07, 5.35MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 452M/809M [01:16<00:56, 6.37MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 453M/809M [01:16<00:44, 8.03MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 455M/809M [01:16<00:41, 8.54MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 456M/809M [01:16<00:42, 8.31MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 457M/809M [01:17<01:49, 3.21MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 458M/809M [01:17<01:36, 3.63MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 458M/809M [01:17<01:25, 4.12MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 460M/809M [01:17<01:05, 5.36MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 461M/809M [01:17<01:01, 5.69MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 463M/809M [01:17<00:43, 7.93MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 464M/809M [01:18<01:11, 4.80MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 466M/809M [01:18<00:51, 6.72MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 467M/809M [01:18<00:43, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 469M/809M [01:18<00:36, 9.24MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 470M/809M [01:18<00:33, 10.1MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 472M/809M [01:19<00:48, 6.89MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 473M/809M [01:20<01:54, 2.94MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 474M/809M [01:20<01:46, 3.15MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 474M/809M [01:20<01:37, 3.43MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 475M/809M [01:20<01:30, 3.70MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 476M/809M [01:20<01:12, 4.57MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 477M/809M [01:20<01:16, 4.33MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 478M/809M [01:21<00:52, 6.29MB/s]\u001b[A\n",
      "model.safetensors:  59%|        | 480M/809M [01:21<01:22, 3.99MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 482M/809M [01:21<01:02, 5.23MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 482M/809M [01:22<01:03, 5.19MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 483M/809M [01:22<01:00, 5.35MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  60%|        | 484M/809M [01:22<00:50, 6.48MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 486M/809M [01:22<00:43, 7.42MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 487M/809M [01:22<00:39, 8.09MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 488M/809M [01:22<00:39, 8.23MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 489M/809M [01:23<01:37, 3.29MB/s]\u001b[A\n",
      "model.safetensors:  61%|        | 490M/809M [01:23<01:41, 3.14MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 491M/809M [01:23<01:20, 3.97MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 492M/809M [01:23<01:02, 5.05MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 494M/809M [01:24<00:45, 6.87MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 495M/809M [01:24<00:39, 7.87MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 496M/809M [01:24<01:24, 3.71MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 498M/809M [01:24<00:54, 5.73MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 499M/809M [01:25<00:46, 6.61MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 501M/809M [01:25<00:39, 7.86MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 502M/809M [01:25<00:33, 9.06MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 504M/809M [01:25<00:30, 9.87MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 505M/809M [01:25<00:27, 11.0MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 507M/809M [01:25<00:24, 12.1MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 509M/809M [01:25<00:23, 12.7MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 510M/809M [01:25<00:22, 13.1MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 512M/809M [01:25<00:21, 13.7MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 513M/809M [01:26<00:39, 7.44MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 514M/809M [01:26<00:36, 8.06MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 516M/809M [01:26<00:32, 9.07MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 517M/809M [01:26<00:31, 9.38MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 519M/809M [01:26<00:28, 10.3MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 520M/809M [01:26<00:28, 10.2MB/s]\u001b[A\n",
      "model.safetensors:  64%|       | 521M/809M [01:27<01:18, 3.68MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 522M/809M [01:28<01:17, 3.70MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 523M/809M [01:28<01:04, 4.43MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 524M/809M [01:28<00:51, 5.49MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 526M/809M [01:28<00:40, 6.99MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 527M/809M [01:28<00:36, 7.78MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 528M/809M [01:29<01:04, 4.33MB/s]\u001b[A\n",
      "model.safetensors:  66%|       | 530M/809M [01:29<00:42, 6.52MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 532M/809M [01:29<00:35, 7.77MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 533M/809M [01:29<00:30, 9.16MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 535M/809M [01:29<00:26, 10.2MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 536M/809M [01:29<00:35, 7.66MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 537M/809M [01:30<01:12, 3.74MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 538M/809M [01:30<01:18, 3.44MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 540M/809M [01:30<00:56, 4.80MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 541M/809M [01:31<00:41, 6.49MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 543M/809M [01:31<00:33, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 544M/809M [01:31<00:54, 4.85MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 546M/809M [01:31<00:43, 6.06MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 548M/809M [01:32<00:36, 7.23MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 550M/809M [01:32<00:30, 8.46MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 551M/809M [01:32<00:28, 9.19MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 552M/809M [01:32<00:28, 8.97MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 553M/809M [01:33<01:15, 3.38MB/s]\u001b[A\n",
      "model.safetensors:  68%|      | 554M/809M [01:33<01:07, 3.78MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 555M/809M [01:33<01:08, 3.70MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 557M/809M [01:33<00:45, 5.58MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 558M/809M [01:33<00:38, 6.60MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 560M/809M [01:34<00:30, 8.25MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 561M/809M [01:34<00:46, 5.32MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 562M/809M [01:34<00:42, 5.84MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 563M/809M [01:34<00:34, 7.20MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 565M/809M [01:34<00:26, 9.19MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 567M/809M [01:34<00:23, 10.3MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 568M/809M [01:35<00:28, 8.35MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 569M/809M [01:36<01:07, 3.54MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 570M/809M [01:36<01:08, 3.50MB/s]\u001b[A\n",
      "model.safetensors:  71%|      | 571M/809M [01:36<01:00, 3.94MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 572M/809M [01:36<00:46, 5.15MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 573M/809M [01:36<00:47, 4.99MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 574M/809M [01:36<00:48, 4.87MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 575M/809M [01:36<00:33, 7.00MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 576M/809M [01:37<00:46, 5.01MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 578M/809M [01:37<00:36, 6.39MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 579M/809M [01:37<00:29, 7.85MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 580M/809M [01:37<00:27, 8.35MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 581M/809M [01:37<00:33, 6.73MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 582M/809M [01:37<00:33, 6.78MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 583M/809M [01:38<00:31, 7.24MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 584M/809M [01:38<00:31, 7.18MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 585M/809M [01:38<01:12, 3.10MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 586M/809M [01:38<01:05, 3.41MB/s]\u001b[A\n",
      "model.safetensors:  72%|     | 586M/809M [01:39<00:58, 3.84MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 587M/809M [01:39<00:45, 4.87MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 588M/809M [01:39<00:35, 6.14MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 590M/809M [01:39<00:31, 6.94MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 591M/809M [01:39<00:25, 8.66MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 592M/809M [01:39<00:42, 5.11MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 594M/809M [01:40<00:29, 7.41MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 595M/809M [01:40<00:26, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 596M/809M [01:40<00:27, 7.77MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 598M/809M [01:40<00:24, 8.58MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 599M/809M [01:40<00:21, 10.0MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 600M/809M [01:40<00:19, 10.5MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 602M/809M [01:40<00:20, 10.0MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 603M/809M [01:40<00:19, 10.6MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 604M/809M [01:40<00:19, 10.8MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 605M/809M [01:41<00:18, 10.9MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 606M/809M [01:41<00:18, 11.2MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 607M/809M [01:41<00:17, 11.3MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 609M/809M [01:41<00:35, 5.69MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  75%|     | 610M/809M [01:41<00:37, 5.32MB/s]\u001b[A\n",
      "model.safetensors:  76%|     | 611M/809M [01:42<00:30, 6.60MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 612M/809M [01:42<00:26, 7.47MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 614M/809M [01:42<00:26, 7.47MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 615M/809M [01:42<00:24, 7.94MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 617M/809M [01:42<00:20, 9.32MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 618M/809M [01:43<00:51, 3.75MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 619M/809M [01:43<00:46, 4.10MB/s]\u001b[A\n",
      "model.safetensors:  77%|    | 620M/809M [01:43<00:39, 4.81MB/s]\u001b[A\n",
      "model.safetensors:  77%|    | 623M/809M [01:43<00:25, 7.38MB/s]\u001b[A\n",
      "model.safetensors:  77%|    | 624M/809M [01:44<00:34, 5.38MB/s]\u001b[A\n",
      "model.safetensors:  77%|    | 626M/809M [01:44<00:24, 7.39MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 628M/809M [01:44<00:21, 8.33MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 630M/809M [01:44<00:18, 9.93MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 631M/809M [01:44<00:16, 10.9MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 633M/809M [01:44<00:15, 11.7MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 634M/809M [01:45<00:14, 11.9MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 636M/809M [01:45<00:14, 12.4MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 637M/809M [01:45<00:13, 12.6MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 638M/809M [01:45<00:15, 11.3MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 640M/809M [01:45<00:15, 10.8MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 641M/809M [01:45<00:26, 6.46MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 642M/809M [01:46<00:25, 6.50MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 643M/809M [01:46<00:29, 5.68MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 644M/809M [01:46<00:20, 8.01MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 646M/809M [01:46<00:19, 8.27MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 647M/809M [01:46<00:21, 7.44MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 648M/809M [01:46<00:17, 8.97MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 649M/809M [01:46<00:17, 9.28MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 651M/809M [01:47<00:15, 10.1MB/s]\u001b[A\n",
      "model.safetensors:  81%|    | 652M/809M [01:47<00:16, 9.40MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 653M/809M [01:47<00:16, 9.63MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 654M/809M [01:47<00:18, 8.56MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 655M/809M [01:47<00:16, 9.18MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 656M/809M [01:47<00:17, 8.89MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 657M/809M [01:48<00:31, 4.77MB/s]\u001b[A\n",
      "model.safetensors:  81%|   | 659M/809M [01:48<00:22, 6.81MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 660M/809M [01:48<00:18, 7.88MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 662M/809M [01:48<00:16, 9.14MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 663M/809M [01:48<00:13, 10.6MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 665M/809M [01:48<00:13, 11.0MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 666M/809M [01:48<00:12, 11.9MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 667M/809M [01:48<00:11, 11.9MB/s]\u001b[A\n",
      "model.safetensors:  83%|   | 669M/809M [01:48<00:10, 13.1MB/s]\u001b[A\n",
      "model.safetensors:  83%|   | 671M/809M [01:49<00:10, 13.4MB/s]\u001b[A\n",
      "model.safetensors:  83%|   | 672M/809M [01:49<00:20, 6.54MB/s]\u001b[A\n",
      "model.safetensors:  83%|   | 674M/809M [01:49<00:15, 8.63MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 676M/809M [01:49<00:14, 9.53MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 677M/809M [01:49<00:13, 10.1MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 678M/809M [01:50<00:12, 10.3MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 680M/809M [01:50<00:10, 12.4MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 682M/809M [01:50<00:10, 12.0MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 684M/809M [01:50<00:09, 12.6MB/s]\u001b[A\n",
      "model.safetensors:  85%|   | 685M/809M [01:50<00:08, 14.2MB/s]\u001b[A\n",
      "model.safetensors:  85%|   | 687M/809M [01:50<00:08, 14.2MB/s]\u001b[A\n",
      "model.safetensors:  85%|   | 688M/809M [01:50<00:14, 8.15MB/s]\u001b[A\n",
      "model.safetensors:  85%|   | 690M/809M [01:51<00:12, 9.37MB/s]\u001b[A\n",
      "model.safetensors:  86%|   | 692M/809M [01:51<00:09, 11.7MB/s]\u001b[A\n",
      "model.safetensors:  86%|  | 694M/809M [01:51<00:09, 12.1MB/s]\u001b[A\n",
      "model.safetensors:  86%|  | 697M/809M [01:51<00:06, 16.1MB/s]\u001b[A\n",
      "model.safetensors:  86%|  | 699M/809M [01:51<00:07, 15.6MB/s]\u001b[A\n",
      "model.safetensors:  87%|  | 700M/809M [01:51<00:07, 13.7MB/s]\u001b[A\n",
      "model.safetensors:  87%|  | 702M/809M [01:51<00:07, 15.0MB/s]\u001b[A\n",
      "model.safetensors:  87%|  | 704M/809M [01:52<00:14, 7.43MB/s]\u001b[A\n",
      "model.safetensors:  87%|  | 707M/809M [01:52<00:09, 11.0MB/s]\u001b[A\n",
      "model.safetensors:  88%|  | 709M/809M [01:52<00:08, 11.8MB/s]\u001b[A\n",
      "model.safetensors:  88%|  | 711M/809M [01:52<00:07, 12.7MB/s]\u001b[A\n",
      "model.safetensors:  88%|  | 713M/809M [01:52<00:07, 12.5MB/s]\u001b[A\n",
      "model.safetensors:  88%|  | 715M/809M [01:52<00:06, 14.5MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 717M/809M [01:53<00:06, 14.2MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 718M/809M [01:53<00:06, 14.3MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 720M/809M [01:53<00:11, 7.84MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 723M/809M [01:53<00:08, 10.4MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 724M/809M [01:53<00:08, 10.6MB/s]\u001b[A\n",
      "model.safetensors:  90%|  | 725M/809M [01:54<00:08, 9.88MB/s]\u001b[A\n",
      "model.safetensors:  90%|  | 727M/809M [01:54<00:07, 10.6MB/s]\u001b[A\n",
      "model.safetensors:  90%|  | 728M/809M [01:54<00:07, 10.4MB/s]\u001b[A\n",
      "model.safetensors:  90%|  | 729M/809M [01:54<00:08, 9.12MB/s]\u001b[A\n",
      "model.safetensors:  90%|  | 732M/809M [01:54<00:06, 12.1MB/s]\u001b[A\n",
      "model.safetensors:  91%| | 734M/809M [01:54<00:06, 12.3MB/s]\u001b[A\n",
      "model.safetensors:  91%| | 735M/809M [01:54<00:06, 12.3MB/s]\u001b[A\n",
      "model.safetensors:  91%| | 736M/809M [01:55<00:10, 6.89MB/s]\u001b[A\n",
      "model.safetensors:  91%| | 739M/809M [01:55<00:07, 9.74MB/s]\u001b[A\n",
      "model.safetensors:  92%| | 741M/809M [01:55<00:05, 11.6MB/s]\u001b[A\n",
      "model.safetensors:  92%| | 743M/809M [01:55<00:04, 13.4MB/s]\u001b[A\n",
      "model.safetensors:  92%| | 745M/809M [01:55<00:05, 12.2MB/s]\u001b[A\n",
      "model.safetensors:  92%| | 747M/809M [01:55<00:04, 13.3MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 749M/809M [01:56<00:04, 14.3MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 750M/809M [01:56<00:04, 14.5MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 752M/809M [01:56<00:04, 12.7MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 753M/809M [01:56<00:07, 7.85MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 755M/809M [01:56<00:05, 9.56MB/s]\u001b[A\n",
      "model.safetensors:  94%| | 757M/809M [01:56<00:04, 11.4MB/s]\u001b[A\n",
      "model.safetensors:  94%| | 759M/809M [01:57<00:04, 11.1MB/s]\u001b[A\n",
      "model.safetensors:  94%| | 761M/809M [01:57<00:03, 12.4MB/s]\u001b[A\n",
      "model.safetensors:  94%| | 762M/809M [01:57<00:03, 12.7MB/s]\u001b[A\n",
      "model.safetensors:  94%| | 764M/809M [01:57<00:03, 12.9MB/s]\u001b[A\n",
      "model.safetensors:  95%| | 766M/809M [01:57<00:03, 13.0MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  95%| | 768M/809M [01:58<00:05, 7.72MB/s]\u001b[A\n",
      "model.safetensors:  96%| | 774M/809M [01:58<00:02, 14.8MB/s]\u001b[A\n",
      "model.safetensors:  96%|| 776M/809M [01:58<00:02, 13.8MB/s]\u001b[A\n",
      "model.safetensors:  96%|| 778M/809M [01:58<00:02, 13.5MB/s]\u001b[A\n",
      "model.safetensors:  96%|| 780M/809M [01:58<00:02, 13.5MB/s]\u001b[A\n",
      "model.safetensors:  97%|| 781M/809M [01:58<00:01, 14.1MB/s]\u001b[A\n",
      "model.safetensors:  97%|| 783M/809M [01:58<00:02, 12.9MB/s]\u001b[A\n",
      "model.safetensors:  97%|| 784M/809M [01:59<00:03, 6.65MB/s]\u001b[A\n",
      "model.safetensors:  98%|| 790M/809M [01:59<00:01, 12.9MB/s]\u001b[A\n",
      "model.safetensors:  98%|| 792M/809M [01:59<00:01, 13.9MB/s]\u001b[A\n",
      "model.safetensors:  98%|| 794M/809M [01:59<00:01, 14.3MB/s]\u001b[A\n",
      "model.safetensors:  98%|| 796M/809M [01:59<00:00, 14.6MB/s]\u001b[A\n",
      "model.safetensors:  99%|| 798M/809M [02:00<00:00, 14.5MB/s]\u001b[A\n",
      "model.safetensors:  99%|| 800M/809M [02:00<00:00, 15.1MB/s]\u001b[A\n",
      "model.safetensors:  99%|| 801M/809M [02:00<00:01, 6.83MB/s]\u001b[A\n",
      "model.safetensors: 100%|| 805M/809M [02:00<00:00, 11.2MB/s]\u001b[A\n",
      "model.safetensors: 100%|| 809M/809M [02:01<00:00, 6.65MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 0: 100%|| 10/10 [10:00:07<00:00, 3600.73s/it, loss=8.03, v_num=8xv2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing model to the hub after training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sentencepiece.bpe.model: 100%|| 1.30M/1.30M [00:00<00:00, 2.94MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 10/10 [10:00:15<00:00, 3601.52s/it, loss=8.03, v_num=8xv2]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"Sparrow\", name=\"donut_test_2\")\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(\"juliansmidek/donut_test_2\",\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(\"juliansmidek/donut_test_2\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(\"juliansmidek/donut_test_2\",\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=\"cpu\",\n",
    "        devices=1,\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        val_check_interval=config.get(\"val_check_interval\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision='bf16', # we'll use mixed precision\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[PushToHubCallback()],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "preprocessor_config.json: 100%|| 421/421 [00:00<00:00, 40.9kB/s]\u001b[A\n",
      "C:\\Users\\thoma\\Documents\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thoma\\.cache\\huggingface\\hub\\models--juliansmidek--donut_test_2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "tokenizer_config.json: 100%|| 5.00k/5.00k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "sentencepiece.bpe.model: 100%|| 1.30M/1.30M [00:00<00:00, 25.8MB/s]\u001b[A\n",
      "\n",
      "tokenizer.json:   0%|                              | 0.00/4.01M [00:00<?, ?B/s]\u001b[A\n",
      "tokenizer.json: 100%|| 4.01M/4.01M [00:01<00:00, 2.68MB/s]\u001b[A\n",
      "\n",
      "added_tokens.json: 100%|| 635/635 [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "special_tokens_map.json: 100%|| 1.04k/1.04k [00:00<?, ?B/s]\u001b[A\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "config.json: 100%|| 4.90k/4.90k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/809M [00:00<?, ?B/s]\u001b[A\n",
      "model.safetensors:   1%|                  | 10.5M/809M [00:00<00:33, 23.8MB/s]\u001b[A\n",
      "model.safetensors:   3%|                  | 21.0M/809M [00:00<00:33, 23.5MB/s]\u001b[A\n",
      "model.safetensors:   4%|                  | 31.5M/809M [00:01<00:31, 24.4MB/s]\u001b[A\n",
      "model.safetensors:   5%|                  | 41.9M/809M [00:01<00:31, 24.4MB/s]\u001b[A\n",
      "model.safetensors:   6%|                 | 52.4M/809M [00:02<00:30, 24.8MB/s]\u001b[A\n",
      "model.safetensors:   8%|                 | 62.9M/809M [00:02<00:29, 25.1MB/s]\u001b[A\n",
      "model.safetensors:   9%|                 | 73.4M/809M [00:02<00:29, 24.6MB/s]\u001b[A\n",
      "model.safetensors:  10%|                 | 83.9M/809M [00:03<00:29, 24.8MB/s]\u001b[A\n",
      "model.safetensors:  12%|                | 94.4M/809M [00:03<00:28, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  13%|                 | 105M/809M [00:04<00:28, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  14%|                 | 115M/809M [00:04<00:27, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  16%|                 | 126M/809M [00:05<00:26, 25.4MB/s]\u001b[A\n",
      "model.safetensors:  17%|                | 136M/809M [00:05<00:26, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  18%|                | 147M/809M [00:05<00:25, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  19%|                | 157M/809M [00:06<00:25, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  21%|               | 168M/809M [00:06<00:25, 24.7MB/s]\u001b[A\n",
      "model.safetensors:  22%|               | 178M/809M [00:07<00:25, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  23%|               | 189M/809M [00:07<00:25, 24.6MB/s]\u001b[A\n",
      "model.safetensors:  25%|               | 199M/809M [00:08<00:24, 24.7MB/s]\u001b[A\n",
      "model.safetensors:  26%|              | 210M/809M [00:08<00:23, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  27%|              | 220M/809M [00:08<00:23, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  29%|              | 231M/809M [00:09<00:23, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  30%|              | 241M/809M [00:09<00:22, 24.8MB/s]\u001b[A\n",
      "model.safetensors:  31%|             | 252M/809M [00:10<00:22, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  32%|             | 262M/809M [00:10<00:21, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  34%|             | 273M/809M [00:10<00:21, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  35%|             | 283M/809M [00:11<00:20, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  36%|            | 294M/809M [00:11<00:20, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  38%|            | 304M/809M [00:12<00:20, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  39%|            | 315M/809M [00:12<00:20, 24.7MB/s]\u001b[A\n",
      "model.safetensors:  40%|            | 325M/809M [00:13<00:19, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  41%|           | 336M/809M [00:13<00:19, 24.8MB/s]\u001b[A\n",
      "model.safetensors:  43%|           | 346M/809M [00:13<00:18, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  44%|           | 357M/809M [00:14<00:17, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  45%|           | 367M/809M [00:14<00:17, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  47%|          | 377M/809M [00:15<00:17, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  48%|          | 388M/809M [00:15<00:16, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  49%|          | 398M/809M [00:15<00:16, 25.4MB/s]\u001b[A\n",
      "model.safetensors:  51%|          | 409M/809M [00:16<00:15, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  52%|         | 419M/809M [00:16<00:15, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  53%|         | 430M/809M [00:17<00:15, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  54%|         | 440M/809M [00:17<00:14, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  56%|        | 451M/809M [00:18<00:14, 24.3MB/s]\u001b[A\n",
      "model.safetensors:  57%|        | 461M/809M [00:18<00:13, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  58%|        | 472M/809M [00:18<00:13, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  60%|        | 482M/809M [00:19<00:12, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  61%|       | 493M/809M [00:19<00:12, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  62%|       | 503M/809M [00:20<00:12, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  63%|       | 514M/809M [00:20<00:11, 24.9MB/s]\u001b[A\n",
      "model.safetensors:  65%|       | 524M/809M [00:20<00:11, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  66%|      | 535M/809M [00:21<00:10, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  67%|      | 545M/809M [00:21<00:10, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  69%|      | 556M/809M [00:22<00:09, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  70%|      | 566M/809M [00:22<00:09, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  71%|     | 577M/809M [00:23<00:09, 24.9MB/s]\u001b[A\n",
      "model.safetensors:  73%|     | 587M/809M [00:23<00:08, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  74%|     | 598M/809M [00:23<00:08, 24.9MB/s]\u001b[A\n",
      "model.safetensors:  75%|     | 608M/809M [00:24<00:08, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  76%|    | 619M/809M [00:24<00:07, 24.5MB/s]\u001b[A\n",
      "model.safetensors:  78%|    | 629M/809M [00:25<00:07, 24.7MB/s]\u001b[A\n",
      "model.safetensors:  79%|    | 640M/809M [00:25<00:06, 25.3MB/s]\u001b[A\n",
      "model.safetensors:  80%|    | 650M/809M [00:25<00:06, 25.6MB/s]\u001b[A\n",
      "model.safetensors:  82%|   | 661M/809M [00:26<00:05, 25.4MB/s]\u001b[A\n",
      "model.safetensors:  83%|   | 671M/809M [00:26<00:05, 25.4MB/s]\u001b[A\n",
      "model.safetensors:  84%|   | 682M/809M [00:27<00:05, 24.2MB/s]\u001b[A\n",
      "model.safetensors:  86%|   | 692M/809M [00:27<00:04, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  87%|  | 703M/809M [00:28<00:04, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  88%|  | 713M/809M [00:28<00:03, 25.0MB/s]\u001b[A\n",
      "model.safetensors:  89%|  | 724M/809M [00:28<00:03, 25.2MB/s]\u001b[A\n",
      "model.safetensors:  91%| | 734M/809M [00:29<00:02, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  92%| | 744M/809M [00:29<00:02, 25.5MB/s]\u001b[A\n",
      "model.safetensors:  93%| | 755M/809M [00:30<00:02, 25.1MB/s]\u001b[A\n",
      "model.safetensors:  95%| | 765M/809M [00:30<00:01, 24.6MB/s]\u001b[A\n",
      "model.safetensors:  96%|| 776M/809M [00:30<00:01, 25.9MB/s]\u001b[A\n",
      "model.safetensors:  97%|| 786M/809M [00:31<00:00, 25.7MB/s]\u001b[A\n",
      "model.safetensors:  98%|| 797M/809M [00:31<00:00, 25.3MB/s]\u001b[A\n",
      "model.safetensors: 100%|| 809M/809M [00:32<00:00, 25.1MB/s]\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation_config.json: 100%|| 221/221 [00:00<?, ?B/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "processor = DonutProcessor.from_pretrained(\"juliansmidek/donut_test_2\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"juliansmidek/donut_test_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                   | 0/10 [00:00<?, ?it/s]\u001b[AC:\\Users\\thoma\\Documents\\GitHub\\sparrow_pdss\\sparrow-ml\\donut\\venv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "\n",
      " 10%|                                      | 1/10 [01:03<09:31, 63.55s/it]\u001b[A\n",
      " 20%|                                  | 2/10 [02:07<08:27, 63.50s/it]\u001b[A\n",
      " 30%|                              | 3/10 [03:10<07:24, 63.54s/it]\u001b[A\n",
      " 40%|                         | 4/10 [04:13<06:20, 63.45s/it]\u001b[A\n",
      " 50%|                     | 5/10 [05:18<05:18, 63.68s/it]\u001b[A\n",
      " 60%|                 | 6/10 [06:21<04:14, 63.67s/it]\u001b[A\n",
      " 70%|             | 7/10 [07:25<03:11, 63.84s/it]\u001b[A\n",
      " 80%|        | 8/10 [08:29<02:07, 63.76s/it]\u001b[A\n",
      " 90%|    | 9/10 [09:33<01:03, 63.84s/it]\u001b[A\n",
      "100%|| 10/10 [10:37<00:00, 63.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracies': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'mean_accuracy': 0.0} length : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from donut import JSONParseEvaluator\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "output_list = []\n",
    "accs = []\n",
    "\n",
    "dataset = load_dataset(\"juliansmidek/donut_test\", split=\"test\")\n",
    "\n",
    "for idx, sample in tqdm(enumerate(dataset), total=len(dataset)):\n",
    "    # prepare encoder inputs\n",
    "    pixel_values = processor(sample[\"image\"].convert(\"RGB\"), return_tensors=\"pt\").pixel_values\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    # prepare decoder inputs\n",
    "    task_prompt = \"<s_cord-v2>\"\n",
    "    decoder_input_ids = processor.tokenizer(task_prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "    decoder_input_ids = decoder_input_ids.to(device)\n",
    "\n",
    "    # autoregressively generate sequence\n",
    "    outputs = model.generate(\n",
    "            pixel_values,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            max_length=model.decoder.config.max_position_embeddings,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            num_beams=1,\n",
    "            bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "\n",
    "    # turn into JSON\n",
    "    seq = processor.batch_decode(outputs.sequences)[0]\n",
    "    seq = seq.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "    seq = re.sub(r\"<.*?>\", \"\", seq, count=1).strip()  # remove first task start token\n",
    "    seq = processor.token2json(seq)\n",
    "\n",
    "    ground_truth = json.loads(sample[\"ground_truth\"])\n",
    "    ground_truth = ground_truth[\"gt_parse\"]\n",
    "    evaluator = JSONParseEvaluator()\n",
    "    score = evaluator.cal_acc(seq, ground_truth)\n",
    "\n",
    "    accs.append(score)\n",
    "    output_list.append(seq)\n",
    "\n",
    "scores = {\"accuracies\": accs, \"mean_accuracy\": np.mean(accs)}\n",
    "print(scores, f\"length : {len(accs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean accuracy:\", np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8-env",
   "language": "python",
   "name": "py3.8-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
